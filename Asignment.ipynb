{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fbc905",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_scoregg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0aaf6",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd014cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca2edf",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8d99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x1      1000 non-null   float64\n",
      " 1   x2      1000 non-null   float64\n",
      " 2   x3      1000 non-null   float64\n",
      " 3   x4      1000 non-null   object \n",
      " 4   x5      513 non-null    object \n",
      " 5   x6      850 non-null    float64\n",
      " 6   x7      1000 non-null   float64\n",
      " 7   x8      1000 non-null   float64\n",
      " 8   x9      1000 non-null   float64\n",
      " 9   x10     1000 non-null   float64\n",
      " 10  x11     1000 non-null   float64\n",
      " 11  x12     1000 non-null   float64\n",
      " 12  x13     1000 non-null   float64\n",
      " 13  x14     1000 non-null   float64\n",
      " 14  x15     1000 non-null   float64\n",
      " 15  x16     1000 non-null   object \n",
      " 16  x17     1000 non-null   float64\n",
      " 17  x18     1000 non-null   object \n",
      " 18  x19     864 non-null    float64\n",
      " 19  x20     1000 non-null   object \n",
      " 20  x21     1000 non-null   float64\n",
      " 21  x22     1000 non-null   float64\n",
      " 22  x23     1000 non-null   float64\n",
      " 23  x24     1000 non-null   float64\n",
      " 24  x25     1000 non-null   float64\n",
      " 25  x26     1000 non-null   float64\n",
      " 26  x27     1000 non-null   float64\n",
      " 27  x28     1000 non-null   float64\n",
      " 28  x29     1000 non-null   float64\n",
      " 29  x30     1000 non-null   float64\n",
      " 30  x31     1000 non-null   float64\n",
      " 31  x32     1000 non-null   float64\n",
      " 32  x33     1000 non-null   float64\n",
      " 33  x34     1000 non-null   float64\n",
      " 34  x35     1000 non-null   float64\n",
      " 35  x36     1000 non-null   float64\n",
      " 36  x37     1000 non-null   float64\n",
      " 37  x38     1000 non-null   float64\n",
      " 38  x39     1000 non-null   float64\n",
      " 39  x40     1000 non-null   float64\n",
      " 40  x41     1000 non-null   float64\n",
      " 41  x42     1000 non-null   float64\n",
      " 42  x43     1000 non-null   float64\n",
      " 43  x44     1000 non-null   float64\n",
      " 44  x45     1000 non-null   float64\n",
      " 45  x46     1000 non-null   float64\n",
      " 46  x47     1000 non-null   float64\n",
      " 47  x48     1000 non-null   float64\n",
      " 48  x49     1000 non-null   float64\n",
      " 49  x50     1000 non-null   float64\n",
      " 50  x51     1000 non-null   float64\n",
      " 51  x52     1000 non-null   float64\n",
      " 52  x53     1000 non-null   float64\n",
      " 53  x54     1000 non-null   float64\n",
      " 54  x55     1000 non-null   float64\n",
      " 55  x56     1000 non-null   float64\n",
      " 56  x57     1000 non-null   float64\n",
      " 57  x58     1000 non-null   float64\n",
      " 58  x59     1000 non-null   float64\n",
      " 59  x60     822 non-null    float64\n",
      " 60  y       1000 non-null   int64  \n",
      "dtypes: float64(55), int64(1), object(5)\n",
      "memory usage: 476.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7237cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>...</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>822.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.027520</td>\n",
       "      <td>0.513983</td>\n",
       "      <td>-0.090537</td>\n",
       "      <td>-1.210440</td>\n",
       "      <td>-0.016750</td>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>-0.275223</td>\n",
       "      <td>-0.008048</td>\n",
       "      <td>-0.517808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>-0.014252</td>\n",
       "      <td>-0.517808</td>\n",
       "      <td>0.048355</td>\n",
       "      <td>0.047588</td>\n",
       "      <td>0.013693</td>\n",
       "      <td>-0.227753</td>\n",
       "      <td>-0.053468</td>\n",
       "      <td>0.083298</td>\n",
       "      <td>0.19300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.121035</td>\n",
       "      <td>3.242106</td>\n",
       "      <td>2.148778</td>\n",
       "      <td>1.849584</td>\n",
       "      <td>1.015740</td>\n",
       "      <td>1.025354</td>\n",
       "      <td>1.033009</td>\n",
       "      <td>4.144597</td>\n",
       "      <td>0.984675</td>\n",
       "      <td>5.066099</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019683</td>\n",
       "      <td>0.997306</td>\n",
       "      <td>5.066099</td>\n",
       "      <td>1.019056</td>\n",
       "      <td>1.013647</td>\n",
       "      <td>1.023539</td>\n",
       "      <td>2.468907</td>\n",
       "      <td>2.033750</td>\n",
       "      <td>1.035480</td>\n",
       "      <td>0.39485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6.099206</td>\n",
       "      <td>-10.391957</td>\n",
       "      <td>-9.900032</td>\n",
       "      <td>-9.037725</td>\n",
       "      <td>-3.666805</td>\n",
       "      <td>-3.105765</td>\n",
       "      <td>-3.618622</td>\n",
       "      <td>-13.498260</td>\n",
       "      <td>-2.889831</td>\n",
       "      <td>-16.637901</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.162404</td>\n",
       "      <td>-2.917222</td>\n",
       "      <td>-16.637901</td>\n",
       "      <td>-3.095309</td>\n",
       "      <td>-3.236129</td>\n",
       "      <td>-3.356436</td>\n",
       "      <td>-7.901218</td>\n",
       "      <td>-7.232486</td>\n",
       "      <td>-2.751643</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.514951</td>\n",
       "      <td>-1.708530</td>\n",
       "      <td>-1.556960</td>\n",
       "      <td>-2.435937</td>\n",
       "      <td>-0.677575</td>\n",
       "      <td>-0.661585</td>\n",
       "      <td>-0.735213</td>\n",
       "      <td>-2.882376</td>\n",
       "      <td>-0.656283</td>\n",
       "      <td>-3.804147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.656209</td>\n",
       "      <td>-0.687829</td>\n",
       "      <td>-3.804147</td>\n",
       "      <td>-0.604802</td>\n",
       "      <td>-0.632033</td>\n",
       "      <td>-0.633514</td>\n",
       "      <td>-1.855397</td>\n",
       "      <td>-1.393314</td>\n",
       "      <td>-0.644342</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.124508</td>\n",
       "      <td>0.562613</td>\n",
       "      <td>0.033638</td>\n",
       "      <td>-1.176528</td>\n",
       "      <td>-0.028013</td>\n",
       "      <td>-0.058568</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>-0.130196</td>\n",
       "      <td>-0.008283</td>\n",
       "      <td>-0.387942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027994</td>\n",
       "      <td>-0.047266</td>\n",
       "      <td>-0.387942</td>\n",
       "      <td>0.034554</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.047589</td>\n",
       "      <td>-0.220056</td>\n",
       "      <td>-0.019715</td>\n",
       "      <td>0.125144</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.289833</td>\n",
       "      <td>2.713078</td>\n",
       "      <td>1.405191</td>\n",
       "      <td>0.096288</td>\n",
       "      <td>0.630085</td>\n",
       "      <td>0.699194</td>\n",
       "      <td>0.723170</td>\n",
       "      <td>2.350465</td>\n",
       "      <td>0.638198</td>\n",
       "      <td>2.813306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675595</td>\n",
       "      <td>0.706127</td>\n",
       "      <td>2.813306</td>\n",
       "      <td>0.719285</td>\n",
       "      <td>0.708858</td>\n",
       "      <td>0.698909</td>\n",
       "      <td>1.434233</td>\n",
       "      <td>1.432528</td>\n",
       "      <td>0.796340</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.507043</td>\n",
       "      <td>11.290847</td>\n",
       "      <td>6.222848</td>\n",
       "      <td>5.879442</td>\n",
       "      <td>3.275454</td>\n",
       "      <td>3.431538</td>\n",
       "      <td>3.235282</td>\n",
       "      <td>12.368714</td>\n",
       "      <td>3.332967</td>\n",
       "      <td>19.649202</td>\n",
       "      <td>...</td>\n",
       "      <td>3.399904</td>\n",
       "      <td>3.567435</td>\n",
       "      <td>19.649202</td>\n",
       "      <td>3.642269</td>\n",
       "      <td>3.807703</td>\n",
       "      <td>3.458322</td>\n",
       "      <td>7.787713</td>\n",
       "      <td>6.299200</td>\n",
       "      <td>3.590011</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x1           x2           x3          x6           x7  \\\n",
       "count  1000.000000  1000.000000  1000.000000  850.000000  1000.000000   \n",
       "mean     -0.027520     0.513983    -0.090537   -1.210440    -0.016750   \n",
       "std       2.121035     3.242106     2.148778    1.849584     1.015740   \n",
       "min      -6.099206   -10.391957    -9.900032   -9.037725    -3.666805   \n",
       "25%      -1.514951    -1.708530    -1.556960   -2.435937    -0.677575   \n",
       "50%      -0.124508     0.562613     0.033638   -1.176528    -0.028013   \n",
       "75%       1.289833     2.713078     1.405191    0.096288     0.630085   \n",
       "max       6.507043    11.290847     6.222848    5.879442     3.275454   \n",
       "\n",
       "                x8           x9          x10          x11          x12  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean      0.010565     0.001084    -0.275223    -0.008048    -0.517808  ...   \n",
       "std       1.025354     1.033009     4.144597     0.984675     5.066099  ...   \n",
       "min      -3.105765    -3.618622   -13.498260    -2.889831   -16.637901  ...   \n",
       "25%      -0.661585    -0.735213    -2.882376    -0.656283    -3.804147  ...   \n",
       "50%      -0.058568     0.025955    -0.130196    -0.008283    -0.387942  ...   \n",
       "75%       0.699194     0.723170     2.350465     0.638198     2.813306  ...   \n",
       "max       3.431538     3.235282    12.368714     3.332967    19.649202  ...   \n",
       "\n",
       "               x52          x53          x54          x55          x56  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.002936    -0.014252    -0.517808     0.048355     0.047588   \n",
       "std       1.019683     0.997306     5.066099     1.019056     1.013647   \n",
       "min      -3.162404    -2.917222   -16.637901    -3.095309    -3.236129   \n",
       "25%      -0.656209    -0.687829    -3.804147    -0.604802    -0.632033   \n",
       "50%      -0.027994    -0.047266    -0.387942     0.034554     0.010712   \n",
       "75%       0.675595     0.706127     2.813306     0.719285     0.708858   \n",
       "max       3.399904     3.567435    19.649202     3.642269     3.807703   \n",
       "\n",
       "               x57          x58          x59         x60           y  \n",
       "count  1000.000000  1000.000000  1000.000000  822.000000  1000.00000  \n",
       "mean      0.013693    -0.227753    -0.053468    0.083298     0.19300  \n",
       "std       1.023539     2.468907     2.033750    1.035480     0.39485  \n",
       "min      -3.356436    -7.901218    -7.232486   -2.751643     0.00000  \n",
       "25%      -0.633514    -1.855397    -1.393314   -0.644342     0.00000  \n",
       "50%       0.047589    -0.220056    -0.019715    0.125144     0.00000  \n",
       "75%       0.698909     1.434233     1.432528    0.796340     0.00000  \n",
       "max       3.458322     7.787713     6.299200    3.590011     1.00000  \n",
       "\n",
       "[8 rows x 56 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe899f74",
   "metadata": {},
   "source": [
    "# What’s Happening in the Dataset?\n",
    "This dataset has 61 columns: 60 columns labeled x1 through x60 and one column called y. Here’s what’s going on:\n",
    "\n",
    "Lots of Features: The 60 x columns (x1 to x60) are different pieces of information about each item. Think of them as characteristics or measurements that describe something, like height, weight, or color in a simpler example.\n",
    "A Mix of Data Types: Some of these columns have numbers (like 2.42 or -0.95), while others have letters (like a8 or e2). This means some columns are about quantities (numerical), and others are about categories or groups (categorical).\n",
    "Missing Information: Sometimes, there are gaps in the data (shown as consecutive commas, like ,,), meaning some items don’t have values for certain columns.\n",
    "A Goal or Label: The y column seems to be what we’re trying to figure out or predict. It has values like 0 or 1, which suggests it’s a yes/no or true/false type of answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a1d5a",
   "metadata": {},
   "source": [
    "# Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1162d5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1       0\n",
       "x2       0\n",
       "x3       0\n",
       "x4       0\n",
       "x5     487\n",
       "      ... \n",
       "x57      0\n",
       "x58      0\n",
       "x59      0\n",
       "x60    178\n",
       "y        0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba832374",
   "metadata": {},
   "source": [
    "As see  above there are  columns with null values now here i can do 2 steps on is to remove those columns and other is removing null values using dropna() function. or fillna function to fill the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2c1c9",
   "metadata": {},
   "source": [
    "# Removing  null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0c9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c992692",
   "metadata": {},
   "source": [
    "# again checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "211f627e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1     0\n",
       "x2     0\n",
       "x3     0\n",
       "x4     0\n",
       "x5     0\n",
       "      ..\n",
       "x57    0\n",
       "x58    0\n",
       "x59    0\n",
       "x60    0\n",
       "y      0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef992ff0",
   "metadata": {},
   "source": [
    "# Now  is the problem\n",
    "#### here is one problem if there are multiple null values in the dataset so by simply removing them might decrease the size of data set which will effect out prediction accuracy . So we need to handle this situation by replacing the null values with some values which  are closer to the other values \n",
    "\n",
    "### As i mentioned earlier we have two types of data one is numerical and other is categorical so we need to handle them separately\n",
    "\n",
    "for numeical values we will take mean to fill the null values and for categorical values we will take mode to fill the null values \n",
    "\n",
    "# Now here is the Qs why we take mean and mode \n",
    "#### The reason is that mean is the average of all the values and mode is the most frequent value in the dataset so by taking these values we are not changing the distribution of the data set which will effect our prediction accuracy we are just filling the null values with similar values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242565e",
   "metadata": {},
   "source": [
    "# Reloading dataset and again checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fef07dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1       0\n",
       "x2       0\n",
       "x3       0\n",
       "x4       0\n",
       "x5     487\n",
       "      ... \n",
       "x57      0\n",
       "x58      0\n",
       "x59      0\n",
       "x60    178\n",
       "y        0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4df445",
   "metadata": {},
   "source": [
    "# Filling similarv values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90c31fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns (e.g., integers, floats)\n",
    "num_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Categorical columns (e.g., strings, objects)\n",
    "cat_cols = df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b613cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numerical columns with the mean\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].mean())\n",
    "\n",
    "# Fill categorical columns with the mode\n",
    "for col in cat_cols:\n",
    "    if not df[col].mode().empty:  # Check if mode exists\n",
    "        df[col] = df[col].fillna(df[col].mode().iloc[0])  # Use first mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720d4c6",
   "metadata": {},
   "source": [
    "Again checking only for confermation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e881e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1     0\n",
       "x2     0\n",
       "x3     0\n",
       "x4     0\n",
       "x5     0\n",
       "      ..\n",
       "x57    0\n",
       "x58    0\n",
       "x59    0\n",
       "x60    0\n",
       "y      0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null removed  values after filling\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd8f5f4",
   "metadata": {},
   "source": [
    "# Now about Outlyers in dataset\n",
    ". Outliers are data points that significantly differ from the rest of the data, and identifying them is crucial for ensuring accurate analysis or modeling.\n",
    "\n",
    "## Checking for Outliers in Numerical Columns\n",
    "Identify Numerical Columns: Separate the numerical columns from your dataset. As out dataset includes a target variable (e.g., 'y'), exclude it from this process since we are  checking features, not the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f40c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=['number']).columns.drop('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b36035",
   "metadata": {},
   "source": [
    "To check for outliers in a dataset, a common and effective approach is to use the \n",
    "# Interquartile Range (IQR) method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08300009",
   "metadata": {},
   "source": [
    "# Calculate Quartiles and IQR\n",
    "\n",
    "- **Q1 (First Quartile)**: The 25th percentile of the data.\n",
    "- **Q3 (Third Quartile)**: The 75th percentile of the data.\n",
    "- **IQR (Interquartile Range)**: The difference between Q3 and Q1, i.e., \n",
    "  \\[\n",
    "  \\text{IQR} = Q3 - Q1\n",
    "  \\]\n",
    "\n",
    "## Determine Outlier Boundaries\n",
    "\n",
    "- **Lower Bound**: \n",
    "  \\[\n",
    "  \\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR}\n",
    "  \\]\n",
    "- **Upper Bound**: \n",
    "  \\[\n",
    "  \\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR}\n",
    "  \\]\n",
    "\n",
    "Any data point below the lower bound or above the upper bound is considered an outlier.\n",
    "\n",
    "## Identify Outliers\n",
    "\n",
    "Check which values in each numerical column fall outside these bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69fec7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers in x1: 12\n",
      "Outliers in x2: 7\n",
      "Outliers in x3: 6\n",
      "Outliers in x6: 27\n",
      "Outliers in x7: 16\n",
      "Outliers in x8: 12\n",
      "Outliers in x9: 5\n",
      "Outliers in x10: 18\n",
      "Outliers in x11: 6\n",
      "Outliers in x12: 16\n",
      "Outliers in x13: 8\n",
      "Outliers in x14: 7\n",
      "Outliers in x15: 6\n",
      "Outliers in x17: 6\n",
      "Outliers in x19: 30\n",
      "Outliers in x21: 8\n",
      "Outliers in x22: 11\n",
      "Outliers in x23: 13\n",
      "Outliers in x24: 13\n",
      "Outliers in x25: 8\n",
      "Outliers in x26: 5\n",
      "Outliers in x27: 10\n",
      "Outliers in x28: 12\n",
      "Outliers in x29: 3\n",
      "Outliers in x30: 6\n",
      "Outliers in x31: 9\n",
      "Outliers in x32: 15\n",
      "Outliers in x33: 7\n",
      "Outliers in x34: 8\n",
      "Outliers in x35: 1\n",
      "Outliers in x36: 11\n",
      "Outliers in x37: 5\n",
      "Outliers in x38: 10\n",
      "Outliers in x39: 13\n",
      "Outliers in x40: 3\n",
      "Outliers in x41: 14\n",
      "Outliers in x42: 4\n",
      "Outliers in x43: 4\n",
      "Outliers in x44: 6\n",
      "Outliers in x45: 6\n",
      "Outliers in x46: 14\n",
      "Outliers in x47: 16\n",
      "Outliers in x48: 14\n",
      "Outliers in x49: 7\n",
      "Outliers in x50: 2\n",
      "Outliers in x51: 17\n",
      "Outliers in x52: 10\n",
      "Outliers in x53: 4\n",
      "Outliers in x54: 16\n",
      "Outliers in x55: 11\n",
      "Outliers in x56: 9\n",
      "Outliers in x57: 12\n",
      "Outliers in x58: 9\n",
      "Outliers in x59: 3\n",
      "Outliers in x60: 21\n"
     ]
    }
   ],
   "source": [
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"Outliers in {col}: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e892c7",
   "metadata": {},
   "source": [
    "# Handling Outliers \n",
    "- You can remove them \n",
    "- You can clip them within range \n",
    "- You can leave them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70938187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cliping the outliers\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Outliers\n",
    "\n",
    "# Create a scatter plot for each numerical column\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df.index, df[col], alpha=0.5)\n",
    "    plt.title(f'Scatter Plot of {col}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(col)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers in x1: 0\n",
      "Outliers in x2: 0\n",
      "Outliers in x3: 0\n",
      "Outliers in x6: 0\n",
      "Outliers in x7: 0\n",
      "Outliers in x8: 0\n",
      "Outliers in x9: 0\n",
      "Outliers in x10: 0\n",
      "Outliers in x11: 0\n",
      "Outliers in x12: 0\n",
      "Outliers in x13: 0\n",
      "Outliers in x14: 0\n",
      "Outliers in x15: 0\n",
      "Outliers in x17: 0\n",
      "Outliers in x19: 0\n",
      "Outliers in x21: 0\n",
      "Outliers in x22: 0\n",
      "Outliers in x23: 0\n",
      "Outliers in x24: 0\n",
      "Outliers in x25: 0\n",
      "Outliers in x26: 0\n",
      "Outliers in x27: 0\n",
      "Outliers in x28: 0\n",
      "Outliers in x29: 0\n",
      "Outliers in x30: 0\n",
      "Outliers in x31: 0\n",
      "Outliers in x32: 0\n",
      "Outliers in x33: 0\n",
      "Outliers in x34: 0\n",
      "Outliers in x35: 0\n",
      "Outliers in x36: 0\n",
      "Outliers in x37: 0\n",
      "Outliers in x38: 0\n",
      "Outliers in x39: 0\n",
      "Outliers in x40: 0\n",
      "Outliers in x41: 0\n",
      "Outliers in x42: 0\n",
      "Outliers in x43: 0\n",
      "Outliers in x44: 0\n",
      "Outliers in x45: 0\n",
      "Outliers in x46: 0\n",
      "Outliers in x47: 0\n",
      "Outliers in x48: 0\n",
      "Outliers in x49: 0\n",
      "Outliers in x50: 0\n",
      "Outliers in x51: 0\n",
      "Outliers in x52: 0\n",
      "Outliers in x53: 0\n",
      "Outliers in x54: 0\n",
      "Outliers in x55: 0\n",
      "Outliers in x56: 0\n",
      "Outliers in x57: 0\n",
      "Outliers in x58: 0\n",
      "Outliers in x59: 0\n",
      "Outliers in x60: 0\n"
     ]
    }
   ],
   "source": [
    "# outlyers are  clipped\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"Outliers in {col}: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204cd4ee",
   "metadata": {},
   "source": [
    "# Encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee83c95",
   "metadata": {},
   "source": [
    "# What’s the Difference Between Label Encoding and One-Hot Encoding?\n",
    "\n",
    "Both encoding methods convert categorical data (e.g., \"red,\" \"blue,\" \"green\") into numerical formats that machine learning algorithms can process. Here’s how they work:\n",
    "\n",
    "## Label Encoding\n",
    "\n",
    "- **How it works**: Assigns a unique integer to each category in a column.\n",
    "- **Example**: For a \"Color\" column with values \"red,\" \"blue,\" and \"green\":\n",
    "  - \"red\" → 0\n",
    "  - \"blue\" → 1\n",
    "  - \"green\" → 2\n",
    "- **Result**: The column becomes a single numerical column (e.g., [0, 1, 2]).\n",
    "\n",
    "### Pros:\n",
    "- Simple and efficient—keeps the dataset compact with just one column.\n",
    "\n",
    "### Cons:\n",
    "- Introduces an artificial order (e.g., 2 > 1 > 0), which can mislead some algorithms into assuming \"green\" is greater than \"blue,\" even if no such relationship exists.\n",
    "\n",
    "## One-Hot Encoding\n",
    "\n",
    "- **How it works**: Creates a new binary column for each category.\n",
    "- **Example**: For the same \"Color\" column:\n",
    "  - \"Color_red\": [1, 0, 0]\n",
    "  - \"Color_blue\": [0, 1, 0]\n",
    "  - \"Color_green\": [0, 0, 1]\n",
    "- **Result**: Each category gets its own column, with 1s and 0s indicating presence or absence.\n",
    "\n",
    "### Pros:\n",
    "- Avoids implying any order between categories, making it ideal when categories are unrelated.\n",
    "\n",
    "### Cons:\n",
    "- Increases the number of columns, which can be a problem if there are many categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19dc46",
   "metadata": {},
   "source": [
    "# Which Should You Use: Label Encoding or One-Hot Encoding?\n",
    "\n",
    "To choose the right encoding for your dataset, we need to consider two key factors: \n",
    "\n",
    "1. **The Nature of Your Categorical Variables**: \n",
    "   - If your categorical variable has an inherent order (e.g., \"low,\" \"medium,\" \"high\"), label encoding may be more appropriate.\n",
    "   - If your categorical variable is nominal (e.g., \"red,\" \"blue,\" \"green\") with no inherent order, one-hot encoding is typically the better choice.\n",
    "\n",
    "2. **The Machine Learning Algorithm You Plan to Use**: \n",
    "   - Some algorithms, like decision trees and random forests, can handle label encoding well without being misled by the numerical values.\n",
    "   - Other algorithms, such as linear regression or logistic regression, may perform better with one-hot encoding to avoid introducing artificial relationships between categories.\n",
    "\n",
    "By considering these factors, you can make a more informed decision on which encoding method to use for your specific dataset and machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c5c2b",
   "metadata": {},
   "source": [
    "# 1. Nature of Your Categorical Variables\n",
    "\n",
    "Categorical variables can be:\n",
    "\n",
    "- **Nominal**: No inherent order (e.g., colors: \"red,\" \"blue,\" \"green\"; or codes: \"a8,\" \"e2\").\n",
    "- **Ordinal**: Natural order exists (e.g., sizes: \"small,\" \"medium,\" \"large\").\n",
    "\n",
    "From your sample data (e.g., x4 = a8, x5 = e2), these look like nominal categories—codes or identifiers without an obvious order. For nominal data:\n",
    "\n",
    "- **Label Encoding**: Can mislead algorithms by suggesting an order (e.g., \"a8\" = 0, \"a9\" = 1) that doesn’t exist.\n",
    "- **One-Hot Encoding**: Is better because it treats each category independently, avoiding any implied ranking.\n",
    "\n",
    "# 2. Number of Unique Categories\n",
    "\n",
    "If a column has many unique values (e.g., >10), one-hot encoding creates lots of new columns, which can make your dataset large and harder to handle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83b3ad29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x4: 10 unique values\n",
      "x5: 10 unique values\n",
      "x16: 5 unique values\n",
      "x18: 13 unique values\n",
      "x20: 8 unique values\n"
     ]
    }
   ],
   "source": [
    "for col in ['x4', 'x5', 'x16', 'x18', 'x20']:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0fea2",
   "metadata": {},
   "source": [
    "# DOING ONE HOT ECNDOING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64b2d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=['x4', 'x5', 'x16', 'x18', 'x20'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c69c7",
   "metadata": {},
   "source": [
    "# Feature and targe separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fc6c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: all columns except 'y'\n",
    "X = df_encoded.drop('y', axis=1)\n",
    "\n",
    "# Target: only the 'y' column\n",
    "y = df_encoded['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b30a7",
   "metadata": {},
   "source": [
    "# What Are Standardization and Normalization?\n",
    "\n",
    "Both standardization and normalization are techniques to scale numerical features in your dataset so that they have similar ranges or distributions. This is crucial for many machine learning algorithms to perform well. Here’s a breakdown:\n",
    "\n",
    "## Standardization\n",
    "\n",
    "- **What it does**: Transforms numerical features to have a mean of 0 and a standard deviation of 1 (unit variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe5b0d",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "- **What it does**: Scales numerical features to a fixed range, typically [0, 1] or [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c4290",
   "metadata": {},
   "source": [
    "# When to apply \n",
    "Look at the min, max, and std (standard deviation)\n",
    "If columns have very different ranges (e.g., one column ranges from 0 to 1, another from -1000 to 1000), scaling is needed to ensure no single feature dominates.\n",
    "If the ranges are similar (e.g., all columns are roughly 0 to 10), scaling might still help but is less critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c019007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                x1           x2           x3           x6           x7  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
      "mean     -0.030962     0.512570    -0.085835    -1.206895    -0.017642   \n",
      "std       2.107468     3.225262     2.129798     1.628374     1.002733   \n",
      "min      -5.722127    -8.340943    -6.000186    -5.233774    -2.639066   \n",
      "25%      -1.514951    -1.708530    -1.556960    -2.151859    -0.677575   \n",
      "50%      -0.124508     0.562613     0.033638    -1.210440    -0.028013   \n",
      "75%       1.289833     2.713078     1.405191    -0.097249     0.630085   \n",
      "max       5.497010     9.345491     5.848416     2.984667     2.591576   \n",
      "\n",
      "                x8           x9          x10          x11          x12  ...  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
      "mean      0.009872     0.001494    -0.267177    -0.009576    -0.513202  ...   \n",
      "std       1.015882     1.028477     4.097601     0.978016     4.997487  ...   \n",
      "min      -2.702753    -2.922788   -10.731637    -2.598004   -13.730327  ...   \n",
      "25%      -0.661585    -0.735213    -2.882376    -0.656283    -3.804147  ...   \n",
      "50%      -0.058568     0.025955    -0.130196    -0.008283    -0.387942  ...   \n",
      "75%       0.699194     0.723170     2.350465     0.638198     2.813306  ...   \n",
      "max       2.740362     2.910745    10.199727     2.579919    12.739486  ...   \n",
      "\n",
      "               x51          x52          x53          x54          x55  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
      "mean      0.022114     0.000614    -0.015048    -0.513202     0.047632   \n",
      "std       0.969805     1.008622     0.993601     4.997487     1.010214   \n",
      "min      -2.513834    -2.653916    -2.778763   -13.730327    -2.590932   \n",
      "25%      -0.605331    -0.656209    -0.687829    -3.804147    -0.604802   \n",
      "50%       0.015676    -0.027994    -0.047266    -0.387942     0.034554   \n",
      "75%       0.667003     0.675595     0.706127     2.813306     0.719285   \n",
      "max       2.575506     2.673301     2.797060    12.739486     2.705416   \n",
      "\n",
      "               x56          x57          x58          x59          x60  \n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
      "mean      0.046249     0.013870    -0.227800    -0.052007     0.082474  \n",
      "std       1.000852     1.013605     2.453594     2.025442     0.920861  \n",
      "min      -2.643370    -2.632147    -6.789842    -5.632079    -2.147117  \n",
      "25%      -0.632033    -0.633514    -1.855397    -1.393314    -0.466600  \n",
      "50%       0.010712     0.047589    -0.220056    -0.019715     0.083298  \n",
      "75%       0.708858     0.698909     1.434233     1.432528     0.653744  \n",
      "max       2.720195     2.697542     6.368677     5.671293     2.334261  \n",
      "\n",
      "[8 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "# checking ranges\n",
    "num_cols = X.select_dtypes(include=['number']).columns\n",
    "print(X[num_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c4c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[num_cols].hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ff4bf",
   "metadata": {},
   "source": [
    "# Applying StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ecdec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 x1            x2            x3            x6            x7  \\\n",
      "count  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03   \n",
      "mean   3.197442e-17  3.552714e-18  2.131628e-17  1.039169e-16  2.309264e-17   \n",
      "std    1.000500e+00  1.000500e+00  1.000500e+00  1.000500e+00  1.000500e+00   \n",
      "min   -2.701826e+00 -2.746426e+00 -2.778344e+00 -2.474182e+00 -2.615585e+00   \n",
      "25%   -7.045093e-01 -6.890019e-01 -6.910804e-01 -5.806015e-01 -6.584631e-01   \n",
      "50%   -4.440986e-02  1.552366e-02  5.612376e-02 -2.177864e-03 -1.034713e-02   \n",
      "75%    6.270350e-01  6.826142e-01  7.004287e-01  6.817858e-01  6.462852e-01   \n",
      "max    2.624351e+00  2.740038e+00  2.787692e+00  2.575367e+00  2.603408e+00   \n",
      "\n",
      "                 x8            x9           x10           x11           x12  \\\n",
      "count  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03   \n",
      "mean   1.421085e-17  3.552714e-18  3.197442e-17 -1.776357e-17 -2.664535e-17   \n",
      "std    1.000500e+00  1.000500e+00  1.000500e+00  1.000500e+00  1.000500e+00   \n",
      "min   -2.671553e+00 -2.844736e+00 -2.555080e+00 -2.647936e+00 -2.646078e+00   \n",
      "25%   -6.612897e-01 -7.166673e-01 -6.385461e-01 -6.615747e-01 -6.588496e-01   \n",
      "50%   -6.740309e-02  2.379499e-02  3.344634e-02  1.323081e-03  2.507717e-02   \n",
      "75%    6.788858e-01  7.020449e-01  6.391429e-01  6.626659e-01  6.659691e-01   \n",
      "max    2.689149e+00  2.830113e+00  2.555676e+00  2.649027e+00  2.653197e+00   \n",
      "\n",
      "       ...           x51           x52           x53           x54  \\\n",
      "count  ...  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03   \n",
      "mean   ...  7.105427e-18  1.776357e-17 -9.769963e-18 -2.664535e-17   \n",
      "std    ...  1.000500e+00  1.000500e+00  1.000500e+00  1.000500e+00   \n",
      "min    ... -2.616212e+00 -2.633154e+00 -2.782906e+00 -2.646078e+00   \n",
      "25%    ... -6.473043e-01 -6.515344e-01 -6.774528e-01 -6.588496e-01   \n",
      "50%    ... -6.641191e-03 -2.837804e-02 -3.244222e-02  2.507717e-02   \n",
      "75%    ...  6.653008e-01  6.695452e-01  7.261826e-01  6.659691e-01   \n",
      "max    ...  2.634209e+00  2.651164e+00  2.831636e+00  2.653197e+00   \n",
      "\n",
      "                x55           x56           x57           x58           x59  \\\n",
      "count  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03   \n",
      "mean   9.325873e-18  7.549517e-18  1.065814e-17  7.105427e-18 -9.769963e-18   \n",
      "std    1.000500e+00  1.000500e+00  1.000500e+00  1.000500e+00  1.000500e+00   \n",
      "min   -2.613194e+00 -2.688674e+00 -2.611809e+00 -2.675799e+00 -2.756369e+00   \n",
      "25%   -6.461601e-01 -6.780439e-01 -6.390144e-01 -6.636841e-01 -6.625610e-01   \n",
      "50%   -1.295173e-02 -3.552533e-02  3.328251e-02  3.157628e-03  1.595125e-02   \n",
      "75%    6.651956e-01  6.623760e-01  6.761817e-01  6.777258e-01  7.333109e-01   \n",
      "max    2.632229e+00  2.673006e+00  2.648976e+00  2.689841e+00  2.827119e+00   \n",
      "\n",
      "                x60  \n",
      "count  1.000000e+03  \n",
      "mean  -1.998401e-17  \n",
      "std    1.000500e+00  \n",
      "min   -2.422414e+00  \n",
      "25%   -5.965601e-01  \n",
      "50%    8.958868e-04  \n",
      "75%    6.206761e-01  \n",
      "max    2.446530e+00  \n",
      "\n",
      "[8 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "# Applying StandardScaler\n",
    "\n",
    "# Identify numerical columns (exclude one-hot encoded columns if they exist)\n",
    "num_cols = X.select_dtypes(include=['number']).columns\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform numerical columns\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# Check the result\n",
    "print(X[num_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1f030",
   "metadata": {},
   "source": [
    "# Apply normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# # Initialize the scaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Fit and transform numerical columns\n",
    "# X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# # Check the result\n",
    "# print(X[num_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92685e",
   "metadata": {},
   "source": [
    "# Important point \n",
    "5. Save the Scaler for Later\n",
    "If you’re splitting your data into training and test sets (a later step), fit the scaler only on the training data and apply it to the test data to avoid data leakage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19721ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit scaler on training data only\n",
    "# scaler = StandardScaler()\n",
    "# X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "\n",
    "# # Transform test data using the same scaler\n",
    "# X_test[num_cols] = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592012d0",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0957699",
   "metadata": {},
   "source": [
    "# What is SMOTE?\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a technique used to address class imbalance in classification datasets by oversampling the minority class. It generates synthetic samples for the minority class rather than duplicating existing ones, which helps balance the dataset.\n",
    "\n",
    "## How it works:\n",
    "\n",
    "1. For each sample in the minority class, SMOTE finds its \\( k \\)-nearest neighbors (typically \\( k = 5 \\)).\n",
    "2. It creates new synthetic samples by interpolating between the original sample and its neighbors. For example, a new point might be a random point along the line connecting the original sample and a neighbor.\n",
    "3. This increases the number of minority class samples, making the classes more balanced.\n",
    "\n",
    "## Example:\n",
    "\n",
    "If your dataset has 90 samples of class 0 (majority) and 10 samples of class 1 (minority), SMOTE can generate additional synthetic samples for class 1 to balance the dataset (e.g., 90 samples for each class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa504ee2",
   "metadata": {},
   "source": [
    "# When Do You Need SMOTE?\n",
    "\n",
    "SMOTE should be applied in scenarios where:\n",
    "\n",
    "- **Class Imbalance**: Your dataset exhibits a significant disparity in class distribution, with one class having far fewer samples than the other.\n",
    "  \n",
    "- **Binary Classification**: This is especially common in binary classification tasks, where one class (e.g., y = 1) is underrepresented compared to the other (e.g., y = 0).\n",
    "\n",
    "- **Model Training**: It is beneficial to use SMOTE before training your model to ensure that the classifier has enough examples from the minority class to learn from, which can lead to improved performance and generalization.\n",
    "\n",
    "- **Performance Improvement**: If your model struggles to predict the minority class accurately, applying SMOTE can help enhance its ability to recognize and classify these instances effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351a10c",
   "metadata": {},
   "source": [
    "\n",
    "# Signs You Need SMOTE:\n",
    "\n",
    "## Check Class Distribution:\n",
    "\n",
    "1. **Calculate the number of samples for each class in y**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c218cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "0    807\n",
      "1    193\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())\n",
    "#2. **Evaluate the Counts**: If the counts are highly skewed (e.g., 90% class 0, 10% class 1), you have an imbalance.\n",
    "\n",
    "#3. **Rule of Thumb**: A common rule of thumb is that if the minority class is less than 20-30% of the majority class, you should consider addressing the imbalance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9691a",
   "metadata": {},
   "source": [
    "# As you cas see imbalnace exist so we have to apply smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "934aa17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>...</th>\n",
       "      <th>x18_d7</th>\n",
       "      <th>x18_d8</th>\n",
       "      <th>x18_d9</th>\n",
       "      <th>x20_b3</th>\n",
       "      <th>x20_b4</th>\n",
       "      <th>x20_b5</th>\n",
       "      <th>x20_b6</th>\n",
       "      <th>x20_b7</th>\n",
       "      <th>x20_b8</th>\n",
       "      <th>x20_b9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.425846</td>\n",
       "      <td>2.781246</td>\n",
       "      <td>-0.949030</td>\n",
       "      <td>-2.698882</td>\n",
       "      <td>-1.265862</td>\n",
       "      <td>1.303713</td>\n",
       "      <td>-0.623570</td>\n",
       "      <td>-0.383982</td>\n",
       "      <td>-1.178374</td>\n",
       "      <td>1.480919</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.608989</td>\n",
       "      <td>0.758950</td>\n",
       "      <td>1.707175</td>\n",
       "      <td>-2.328976</td>\n",
       "      <td>0.181869</td>\n",
       "      <td>-0.593474</td>\n",
       "      <td>-0.911715</td>\n",
       "      <td>-1.590597</td>\n",
       "      <td>1.201493</td>\n",
       "      <td>-2.200205</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.748042</td>\n",
       "      <td>4.356598</td>\n",
       "      <td>1.263649</td>\n",
       "      <td>0.071027</td>\n",
       "      <td>0.387170</td>\n",
       "      <td>-0.417647</td>\n",
       "      <td>1.098382</td>\n",
       "      <td>-3.460642</td>\n",
       "      <td>0.069232</td>\n",
       "      <td>-5.964430</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.169499</td>\n",
       "      <td>3.348746</td>\n",
       "      <td>0.505379</td>\n",
       "      <td>-4.522803</td>\n",
       "      <td>0.036467</td>\n",
       "      <td>0.676045</td>\n",
       "      <td>0.174182</td>\n",
       "      <td>-6.050133</td>\n",
       "      <td>0.079175</td>\n",
       "      <td>8.502457</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.696148</td>\n",
       "      <td>1.059532</td>\n",
       "      <td>0.160994</td>\n",
       "      <td>-2.293077</td>\n",
       "      <td>1.942567</td>\n",
       "      <td>2.404731</td>\n",
       "      <td>-1.047287</td>\n",
       "      <td>-1.442264</td>\n",
       "      <td>-0.472782</td>\n",
       "      <td>-2.233274</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.398930</td>\n",
       "      <td>-2.319879</td>\n",
       "      <td>1.441124</td>\n",
       "      <td>-1.210440</td>\n",
       "      <td>0.327850</td>\n",
       "      <td>-0.219549</td>\n",
       "      <td>0.407780</td>\n",
       "      <td>-5.114493</td>\n",
       "      <td>-1.092742</td>\n",
       "      <td>3.613092</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3.750240</td>\n",
       "      <td>-1.881061</td>\n",
       "      <td>1.315283</td>\n",
       "      <td>-1.983028</td>\n",
       "      <td>-1.464487</td>\n",
       "      <td>1.199392</td>\n",
       "      <td>0.626827</td>\n",
       "      <td>-2.656070</td>\n",
       "      <td>-0.814035</td>\n",
       "      <td>-1.727417</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>5.497010</td>\n",
       "      <td>-0.204798</td>\n",
       "      <td>-0.098950</td>\n",
       "      <td>-3.549766</td>\n",
       "      <td>0.382518</td>\n",
       "      <td>0.379074</td>\n",
       "      <td>-1.343742</td>\n",
       "      <td>2.937457</td>\n",
       "      <td>-1.081328</td>\n",
       "      <td>0.602280</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1.747577</td>\n",
       "      <td>-1.039617</td>\n",
       "      <td>3.842766</td>\n",
       "      <td>1.498057</td>\n",
       "      <td>0.079344</td>\n",
       "      <td>1.445338</td>\n",
       "      <td>-0.981217</td>\n",
       "      <td>0.023239</td>\n",
       "      <td>-0.844222</td>\n",
       "      <td>-4.193965</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.240898</td>\n",
       "      <td>-2.327819</td>\n",
       "      <td>-2.457964</td>\n",
       "      <td>0.029318</td>\n",
       "      <td>0.478228</td>\n",
       "      <td>0.358742</td>\n",
       "      <td>0.747721</td>\n",
       "      <td>2.435888</td>\n",
       "      <td>0.838707</td>\n",
       "      <td>8.229666</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x6        x7        x8        x9  \\\n",
       "0    2.425846  2.781246 -0.949030 -2.698882 -1.265862  1.303713 -0.623570   \n",
       "1   -0.608989  0.758950  1.707175 -2.328976  0.181869 -0.593474 -0.911715   \n",
       "2    0.748042  4.356598  1.263649  0.071027  0.387170 -0.417647  1.098382   \n",
       "3   -1.169499  3.348746  0.505379 -4.522803  0.036467  0.676045  0.174182   \n",
       "4    3.696148  1.059532  0.160994 -2.293077  1.942567  2.404731 -1.047287   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995 -0.398930 -2.319879  1.441124 -1.210440  0.327850 -0.219549  0.407780   \n",
       "996  3.750240 -1.881061  1.315283 -1.983028 -1.464487  1.199392  0.626827   \n",
       "997  5.497010 -0.204798 -0.098950 -3.549766  0.382518  0.379074 -1.343742   \n",
       "998  1.747577 -1.039617  3.842766  1.498057  0.079344  1.445338 -0.981217   \n",
       "999  1.240898 -2.327819 -2.457964  0.029318  0.478228  0.358742  0.747721   \n",
       "\n",
       "          x10       x11       x12  ...  x18_d7  x18_d8  x18_d9  x20_b3  \\\n",
       "0   -0.383982 -1.178374  1.480919  ...   False   False   False   False   \n",
       "1   -1.590597  1.201493 -2.200205  ...   False   False   False    True   \n",
       "2   -3.460642  0.069232 -5.964430  ...   False   False   False    True   \n",
       "3   -6.050133  0.079175  8.502457  ...   False   False   False   False   \n",
       "4   -1.442264 -0.472782 -2.233274  ...   False   False   False   False   \n",
       "..        ...       ...       ...  ...     ...     ...     ...     ...   \n",
       "995 -5.114493 -1.092742  3.613092  ...   False   False   False    True   \n",
       "996 -2.656070 -0.814035 -1.727417  ...   False   False   False   False   \n",
       "997  2.937457 -1.081328  0.602280  ...   False   False   False   False   \n",
       "998  0.023239 -0.844222 -4.193965  ...   False   False   False    True   \n",
       "999  2.435888  0.838707  8.229666  ...   False   False   False    True   \n",
       "\n",
       "     x20_b4  x20_b5  x20_b6  x20_b7  x20_b8  x20_b9  \n",
       "0     False   False    True   False   False   False  \n",
       "1     False   False   False   False   False   False  \n",
       "2     False   False   False   False   False   False  \n",
       "3     False   False   False    True   False   False  \n",
       "4     False   False   False    True   False   False  \n",
       "..      ...     ...     ...     ...     ...     ...  \n",
       "995   False   False   False   False   False   False  \n",
       "996   False   False   False    True   False   False  \n",
       "997   False   False    True   False   False   False  \n",
       "998   False   False   False   False   False   False  \n",
       "999   False   False   False   False   False   False  \n",
       "\n",
       "[1000 rows x 97 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf984932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "0    807\n",
      "1    807\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X and y are your features and target after preprocessing\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b4826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE:\n",
      "y\n",
      "0    648\n",
      "1    648\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Assuming X and y are your features and target\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "\n",
    "# Transform test data using the same scaler\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Verify class balance\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba9a3504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Training K-Nearest Neighbors...\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training Support Vector Machine...\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\zanib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:40:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      "Model Performance Summary:\n",
      "                        CV Accuracy     CV F1  Test Accuracy  Test Precision  \\\n",
      "Logistic Regression        0.852082  0.803328          0.745        0.352941   \n",
      "K-Nearest Neighbors        0.682109  0.758542          0.420        0.241379   \n",
      "Decision Tree              0.790950  0.794013          0.720        0.317073   \n",
      "Random Forest              0.932174  0.927470          0.820        0.857143   \n",
      "Support Vector Machine     0.943745  0.940902          0.795        0.500000   \n",
      "XGBoost                    0.904446  0.891131          0.825        0.714286   \n",
      "Naive Bayes                0.525462  0.677166          0.230        0.204188   \n",
      "\n",
      "                        Test Recall   Test F1  Test ROC-AUC  \n",
      "Logistic Regression        0.292683  0.320000      0.569412  \n",
      "K-Nearest Neighbors        0.853659  0.376344      0.681316  \n",
      "Decision Tree              0.317073  0.317073      0.570486  \n",
      "Random Forest              0.146341  0.250000      0.736156  \n",
      "Support Vector Machine     0.170732  0.254545      0.731094  \n",
      "XGBoost                    0.243902  0.363636      0.757018  \n",
      "Naive Bayes                0.951220  0.336207      0.512579  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    # Cross-validation scores\n",
    "    cv_accuracy = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    cv_f1 = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else \"N/A\"\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'CV Accuracy': cv_accuracy,\n",
    "        'CV F1': cv_f1,\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Precision': precision,\n",
    "        'Test Recall': recall,\n",
    "        'Test F1': f1,\n",
    "        'Test ROC-AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# List of models\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Support Vector Machine', SVC(probability=True, random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')),\n",
    "    ('Naive Bayes', GaussianNB())\n",
    "]\n",
    "\n",
    "# Apply each model\n",
    "for model_name, model in models:\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    evaluate_model(model, model_name, X_train_resampled, y_train_resampled, X_test, y_test)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
